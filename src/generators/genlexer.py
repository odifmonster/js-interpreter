#!/usr/bin/env python

from typing import TypeAlias
from io import TextIOWrapper

DeltaFunc: TypeAlias = dict[str, dict[str, tuple[str, str, bool]]]

LEX_GRAM = "src/grammar/interp.lex"
LEX_TYPES = "include/lexer/lexTypes.ts"

def get_char_set(set_str: str) -> set[str]:
    chset: set[str] = set()

    i = 0
    prev = ''
    while i < len(set_str):
        if prev and set_str[i] == '-' and i+1 < len(set_str):
            start, end = ord(prev), ord(set_str[i+1])
            for j in range(start, end+1):
                chset.add(chr(j))
            i += 1
            prev = ''
        else:
            chset.add(set_str[i])
            prev = set_str[i]
        i += 1
    
    return chset

def parse_lex_file(fpath: str) -> tuple[DeltaFunc, dict[str, str]]:
    with open(fpath) as infile:
        flines = [line.strip() for line in infile if line.strip() != '']

    delta: dict[str, dict[str, tuple[str, str, bool]]] = {}
    rsvd: dict[str, str] = {}
    in_rsv = False

    for line in flines:
        if 'RESERVED' in line:
            in_rsv = True
            continue

        if in_rsv:
            word, _, tok = line.split()
            rsvd[word] = tok
        else:
            consume = True
            nxt_state = ''
            ret_val = ''

            if line[0] == '[':
                consume = False
                line = line[1:-1]
            
            s, _, ch_str, t_or_ret = line.split()
            if s not in delta: delta[s] = {}

            chset: set[str] = set()
            if ch_str[0] == '[':
                chset = get_char_set(ch_str[1:-1])
            elif ch_str == 'WS': chset = { ' ', '\t', '\n' }
            elif ch_str == 'NEWLINE': chset = { '\n' }
            elif ch_str == 'TAB': chset = { '\t' }
            elif ch_str == 'SPACE': chset = { ' ' }
            else: chset = { ch_str }

            if t_or_ret[0] == '<':
                ret_val = t_or_ret[1:-1]
            else:
                nxt_state = t_or_ret
            
            for c in chset:
                delta[s][c] = (nxt_state, ret_val, consume)
    
    return delta, rsvd

def write_groups(dest: TextIOWrapper, vals: set[str], grp_size = 5) -> None:
    vlist = list(vals)

    for i in range(0, len(vlist), 5):
        grp = vlist[i:i+grp_size]
        dest.write(" | ".join([f"\"{s}\"" for s in grp]))

        if i+grp_size < len(vlist):
            dest.write("\n    | ")
        else:
            dest.write(";")

def add_states_tokens(destpath: str, delta: DeltaFunc, rsvd: dict[str, str]) -> None:
    states: set[str] = set(delta.keys())
    tokens: set[str] = set()

    for val in delta.values():
        for _, tok, _ in val.values():
            if tok: tokens.add(tok)
    
    for val in rsvd.values():
        tokens.add(val)

    outfile = open(destpath, "a")
    outfile.write("\n\n// GENERATED BY src/generators/genlexer.py\n\n")

    outfile.write("export type LexerState = ")
    write_groups(outfile, states)
    
    outfile.write("\n\nexport type TokenKind = ")
    write_groups(outfile, tokens)

    outfile.close()

def main():
    delta, rsvd = parse_lex_file(LEX_GRAM)
    add_states_tokens(LEX_TYPES, delta, rsvd)

if __name__ == "__main__":
    main()
